"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                              â•‘
â•‘   ğŸ•·ï¸  CRAWL ENGINE v2 â€” Investor Lead Machine                â•‘
â•‘                                                              â•‘
â•‘   Config-driven, multi-site crawler with stealth,            â•‘
â•‘   enrichment, scoring, and automated output.                 â•‘
â•‘                                                              â•‘
â•‘   Usage:                                                     â•‘
â•‘     python engine.py                    # Crawl all sites    â•‘
â•‘     python engine.py --site openvc      # Crawl one site     â•‘
â•‘     python engine.py --dry-run          # Test without save  â•‘
â•‘     python engine.py --headless         # No browser window  â•‘
â•‘                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import asyncio
import argparse
import sys
import time
from pathlib import Path
from datetime import datetime

import yaml
from playwright.async_api import async_playwright

# â”€â”€ Internal modules â”€â”€
from adapters.openvc import OpenVCAdapter
from adapters.angelmatch import AngelMatchAdapter
from stealth.fingerprint import FingerprintManager
from stealth.behavior import HumanBehavior
from stealth.proxy import ProxyManager
from enrichment.email_validator import EmailValidator
from enrichment.scoring import LeadScorer
from output.csv_writer import CSVWriter
from output.webhook import WebhookNotifier


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  Adapter Registry
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ADAPTER_MAP = {
    "openvc": OpenVCAdapter,
    "angelmatch": AngelMatchAdapter,
    # Add new adapters here as you build them:
    # "signal": SignalAdapter,
    # "crunchbase": CrunchbaseAdapter,
}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  Engine
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class CrawlEngine:
    """
    Main orchestrator. Wires together:
    - Site configs â†’ Adapters
    - Stealth layer (fingerprints, human behavior, proxies)
    - Enrichment pipeline (email validation, lead scoring)
    - Output (CSV, webhooks)
    """

    def __init__(self, args):
        self.args = args
        self.config = self._load_config("config/sites.yaml")
        self.fingerprint_mgr = FingerprintManager()
        self.behavior = HumanBehavior(speed_factor=1.0)
        self.proxy_mgr = ProxyManager("config/proxies.yaml")
        self.email_validator = EmailValidator()
        self.scorer = LeadScorer("config/scoring.yaml")
        self.csv_writer = CSVWriter("data")
        self.webhook = WebhookNotifier(
            webhook_url=args.webhook or "",
            platform=args.webhook_platform or "discord",
        )
        self.all_leads = []

    def _load_config(self, path: str) -> dict:
        with open(path) as f:
            return yaml.safe_load(f)

    async def run(self):
        """Execute the full crawl pipeline."""
        start_time = time.time()
        self._print_banner()

        sites = self.config.get("sites", {})
        defaults = self.config.get("defaults", {})

        # Filter to specific site if requested
        if self.args.site:
            if self.args.site not in sites:
                print(f"\n  âŒ  Site '{self.args.site}' not found in config.")
                print(f"  Available: {', '.join(sites.keys())}")
                return
            sites = {self.args.site: sites[self.args.site]}

        async with async_playwright() as p:
            for site_name, site_config in sites.items():
                if not site_config.get("enabled", True):
                    print(f"\n  â­ï¸  Skipping {site_name} (disabled)")
                    continue

                adapter_name = site_config.get("adapter", "")
                adapter_class = ADAPTER_MAP.get(adapter_name)

                if not adapter_class:
                    print(f"\n  âš ï¸  No adapter found for '{adapter_name}', skipping {site_name}")
                    continue

                try:
                    leads = await self._crawl_site(p, site_name, site_config, adapter_class, defaults)
                    self.all_leads.extend(leads)
                except Exception as e:
                    print(f"\n  âŒ  Error crawling {site_name}: {e}")
                    if self.args.verbose:
                        import traceback
                        traceback.print_exc()

        # â”€â”€ Post-crawl pipeline â”€â”€
        if self.all_leads:
            await self._enrich_and_output()
        else:
            print("\n  âš ï¸  No leads collected. Check your site configs and selectors.")

        # â”€â”€ Stats â”€â”€
        elapsed = time.time() - start_time
        self._print_summary(elapsed)

    async def _crawl_site(self, playwright, site_name, site_config, adapter_class, defaults):
        """Crawl a single site with a fresh browser context."""
        print(f"\n  ğŸŒ  Initializing browser for {site_name}...")

        # Generate a fresh fingerprint for this site
        fingerprint = self.fingerprint_mgr.generate()
        context_kwargs = self.fingerprint_mgr.get_context_kwargs(fingerprint)

        # Check for proxy
        proxy = self.proxy_mgr.get_proxy(site_name)

        # Determine headless mode
        headless = self.args.headless or defaults.get("headless", False)

        # Launch browser
        browser = await playwright.chromium.launch(headless=headless)
        context = await browser.new_context(
            **context_kwargs,
            **({"proxy": proxy} if proxy else {}),
        )

        # Apply JS fingerprint overrides
        page = await context.new_page()
        await self.fingerprint_mgr.apply_js_overrides(page)

        # Run the adapter
        adapter = adapter_class(site_config, stealth_module=self.behavior)
        leads = await adapter.run(page)

        # Take a screenshot if configured
        if defaults.get("screenshots", False):
            ss_dir = Path(defaults.get("screenshot_dir", "data/screenshots"))
            ss_dir.mkdir(parents=True, exist_ok=True)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            await page.screenshot(
                path=str(ss_dir / f"{site_name}_{timestamp}.png"),
                full_page=True,
            )
            print(f"  ğŸ“¸  Screenshot saved")

        await browser.close()
        return leads

    async def _enrich_and_output(self):
        """Run enrichment and output pipeline on collected leads."""
        print(f"\n{'='*60}")
        print(f"  ğŸ§   ENRICHMENT PIPELINE")
        print(f"{'='*60}\n")

        # â”€â”€ Email validation â”€â”€
        print(f"  ğŸ“§  Validating {len(self.all_leads)} emails...")
        for lead in self.all_leads:
            result = self.email_validator.validate(lead.email)
            if result["quality"] == "invalid":
                lead.email = "N/A (invalid)"
            elif result["is_disposable"]:
                lead.email = f"{lead.email} âš ï¸ (disposable)"

        # â”€â”€ Lead scoring â”€â”€
        print(f"  ğŸ“Š  Scoring leads...")
        self.all_leads = self.scorer.score_batch(self.all_leads)

        # â”€â”€ Delta detection â”€â”€
        deltas = self.csv_writer.detect_deltas(self.all_leads)

        # â”€â”€ Output â”€â”€
        if not self.args.dry_run:
            print(f"\n  ğŸ’¾  Writing output...")
            master_path = self.csv_writer.write_master(self.all_leads)

            # Webhook notifications
            hot_count = sum(1 for l in self.all_leads if l.lead_score >= 80)
            await self.webhook.notify_hot_leads(self.all_leads)
            await self.webhook.notify_crawl_complete(
                total=len(self.all_leads),
                new=len(deltas),
                hot=hot_count,
            )
        else:
            print(f"\n  ğŸ§ª  DRY RUN â€” no files written")

    def _print_banner(self):
        print()
        print("  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
        print("  â•‘   ğŸ•·ï¸  CRAWL ENGINE v2                    â•‘")
        print("  â•‘   Investor Lead Machine                  â•‘")
        print("  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        print()
        print(f"  â°  {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"  ğŸ¯  Sites: {self.args.site or 'ALL'}")
        print(f"  ğŸ‘»  Stealth: ON")
        print(f"  ğŸ”’  Proxy: {'ON' if self.proxy_mgr.enabled else 'OFF'}")
        print(f"  ğŸ–¥ï¸  Headless: {'YES' if self.args.headless else 'NO'}")
        print()

    def _print_summary(self, elapsed: float):
        print(f"\n{'='*60}")
        print(f"  ğŸ“Š  CRAWL SUMMARY")
        print(f"{'='*60}")
        print(f"  â±ï¸  Duration: {elapsed:.1f}s")
        print(f"  ğŸ“  Total leads: {len(self.all_leads)}")

        if self.all_leads:
            scorer_stats = self.scorer.stats
            print(f"  ğŸ“ˆ  Avg score: {scorer_stats.get('avg_score', 0)}")
            print(f"  ğŸ”´  HOT leads: {scorer_stats.get('hot_count', 0)}")
            print(f"  ğŸŸ¡  WARM leads: {scorer_stats.get('warm_count', 0)}")

        fp_stats = self.fingerprint_mgr.stats
        print(f"  ğŸ­  Fingerprints used: {fp_stats['total_fingerprints_generated']}")
        print(f"  ğŸ”’  Proxy requests: {self.proxy_mgr.stats['total_requests_proxied']}")
        print()

        # Top 5 leads preview
        if self.all_leads:
            print(f"  ğŸ†  TOP 5 LEADS:")
            print(f"  {'â”€'*50}")
            for lead in self.all_leads[:5]:
                areas = ", ".join(lead.focus_areas[:2]) if lead.focus_areas else "N/A"
                print(f"  {lead.tier}  {lead.name} ({lead.fund})")
                print(f"       ğŸ“§ {lead.email} | ğŸ¯ {areas}")
                print(f"       ğŸ’° {lead.check_size} | Score: {lead.lead_score}")
                print()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  CLI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def parse_args():
    parser = argparse.ArgumentParser(
        description="ğŸ•·ï¸ CRAWL â€” Investor Lead Machine",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument(
        "--site", type=str, default="",
        help="Crawl a specific site only (e.g. openvc, angelmatch)",
    )
    parser.add_argument(
        "--headless", action="store_true",
        help="Run browser in headless mode (no visible window)",
    )
    parser.add_argument(
        "--dry-run", action="store_true",
        help="Run crawl but don't write output files",
    )
    parser.add_argument(
        "--verbose", action="store_true",
        help="Show detailed error tracebacks",
    )
    parser.add_argument(
        "--webhook", type=str, default="",
        help="Discord/Slack webhook URL for notifications",
    )
    parser.add_argument(
        "--webhook-platform", type=str, default="discord",
        choices=["discord", "slack"],
        help="Webhook platform (default: discord)",
    )
    return parser.parse_args()


async def main():
    args = parse_args()
    engine = CrawlEngine(args)
    await engine.run()


if __name__ == "__main__":
    asyncio.run(main())
